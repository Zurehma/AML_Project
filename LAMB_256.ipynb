{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOueZy/6/jMm+7zMiiHfhX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zurehma/AML_Project/blob/zurehma/LAMB_256.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6E37Gkmf3MB"
      },
      "outputs": [],
      "source": [
        "!pip install torch-optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ─── Import LAMB from torch-optimizer ───────────────────────────────────────\n",
        "from torch_optimizer import Lamb\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "import math\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Seeding for reproducibility\n",
        "seed = 2025\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# transforms\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# load data\n",
        "def load_data(dataset_name=\"cifar100\", batch_size=128, num_workers=2):\n",
        "    \"\"\"Enhanced data loading with stronger augmentation for CIFAR-100\"\"\"\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "    test_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if dataset_name.lower() == \"cifar100\":\n",
        "        train_dataset = CIFAR100(\n",
        "            root=\"./data\", train=True, download=True, transform=train_transform\n",
        "        )\n",
        "        test_dataset = CIFAR100(\n",
        "            root=\"./data\", train=False, download=True, transform=test_transform\n",
        "        )\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset {dataset_name} not supported in enhanced version\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return train_loader, test_loader, num_classes\n",
        "\n",
        "\n",
        "# model - LeNet5 from the paper [11]\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Input: 3x32x32 (CIFAR has 3 color channels)\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)  # Output: 6x28x28\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # Output: 16x10x10\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 16x5x5\n",
        "\n",
        "        # fc layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # using BatchNorm for better training stability\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 384)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        # Second conv block\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        # Flatten and FC\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(model_name, num_classes=100):\n",
        "    if model_name.lower() == \"improved_cnn\":\n",
        "        return ImprovedCNN(num_classes=num_classes)\n",
        "    elif model_name.lower() == \"lenet5\":\n",
        "        return LeNet5(num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "\n",
        "# train\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        pbar.set_postfix({\"loss\": running_loss / total,\n",
        "                          \"acc\": 100.0 * correct / total})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100.0 * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# eval\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = 100.0 * correct / total\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "# train config\n",
        "def train_centralized_model(\n",
        "    model_name=\"improved_cnn\",\n",
        "    dataset_name=\"cifar100\",\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    lr=0.01,\n",
        "    momentum=0.9,\n",
        "    weight_decay=4e-4,\n",
        "    lr_scheduler=\"lamb\",\n",
        "    save_path=\"improved_cnn_model.pth\",\n",
        "):\n",
        "    # Load data\n",
        "    train_loader, test_loader, num_classes = load_data(\n",
        "        dataset_name=dataset_name, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(model_name, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Print total parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Linear‐scaling rule for LR\n",
        "    scaled_lr = lr * (batch_size / 256)\n",
        "\n",
        "    # Separate parameters into decay vs. no‐decay groups\n",
        "    decay_params = []\n",
        "    no_decay_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if param.ndim == 1:\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    # ─── Instantiate LAMB ─────────────────────────────────────────────────────────\n",
        "    #\n",
        "    # We set global weight_decay=0.0 because we're handling it per‐group.\n",
        "    #\n",
        "    optimizer = Lamb(\n",
        "        [\n",
        "            {\"params\": decay_params,    \"weight_decay\": weight_decay},\n",
        "            {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
        "        ],\n",
        "        lr=scaled_lr,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-6,\n",
        "        weight_decay=0.0  # global default; per-group overrides actual decay\n",
        "    )\n",
        "    # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # Learning‐rate scheduler: warmup + cosine, same as LARS code\n",
        "    if lr_scheduler == \"lamb\":\n",
        "        warmup_epochs = 5\n",
        "\n",
        "        def lr_lambda(epoch):\n",
        "            if epoch < warmup_epochs:\n",
        "                return float(epoch + 1) / float(warmup_epochs)\n",
        "            return 0.5 * (\n",
        "                1.0\n",
        "                + math.cos(\n",
        "                    math.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    elif lr_scheduler == \"step\":\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=[int(epochs * 0.5), int(epochs * 0.75)],\n",
        "            gamma=0.1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler: {lr_scheduler}\")\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{epochs}] - Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "            f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, \"\n",
        "            f\"Time: {train_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"test_acc\": test_acc,\n",
        "                },\n",
        "                save_path,\n",
        "            )\n",
        "            print(f\"New best model saved with accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"Best model loaded with accuracy: {checkpoint['test_acc']:.2f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# visualization of model performance\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    ax1.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    ax1.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Loss Curves\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    ax2.plot(history[\"test_acc\"], label=\"Test Accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy (%)\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Accuracy Curves\")\n",
        "    ax2.grid(True)\n",
        "\n",
        "    ax3.plot(history[\"lr\"])\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Learning Rate\")\n",
        "    ax3.set_title(\"Learning Rate Schedule\")\n",
        "    ax3.set_yscale(\"log\")\n",
        "    ax3.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# run benchmark\n",
        "def run_centralized_benchmark():\n",
        "    config = {\n",
        "        \"model_name\": \"improved_cnn\",\n",
        "        \"dataset_name\": \"cifar100\",\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"momentum\": 0.9,\n",
        "        \"weight_decay\": 4e-4,\n",
        "        \"lr_scheduler\": \"lamb\",\n",
        "        \"save_path\": \"improved_cnn_model_lamb.pth\",\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        f\"Running LAMB benchmark with {config['model_name']} on {config['dataset_name']}...\"\n",
        "    )\n",
        "\n",
        "    model, history = train_centralized_model(**config)\n",
        "    plot_training_history(history)\n",
        "\n",
        "    with open(\"history_lamb.pkl\", \"wb\") as f:\n",
        "        pickle.dump(history, f)\n",
        "\n",
        "    print(\"LAMB benchmark completed!\")\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def main():\n",
        "    model, history = run_centralized_benchmark()\n",
        "    return model, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "SZMi8j-9gDLr",
        "outputId": "9357afce-2d2b-4800-f64b-06cc619a7e9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Using device: cpu\n",
            "Running LAMB benchmark with improved_cnn on cifar100...\n",
            "Total parameters: 1,774,052\n",
            "\n",
            "Epoch [1/100] - Learning Rate: 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 196/196 [05:46<00:00,  1.77s/it, loss=4.43, acc=3.47]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.4293, Train Acc: 3.47%, Test Loss: 4.0599, Test Acc: 9.06%, Time: 346.66s\n",
            "New best model saved with accuracy: 9.06%\n",
            "\n",
            "Epoch [2/100] - Learning Rate: 0.004000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 196/196 [05:33<00:00,  1.70s/it, loss=4, acc=8.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9978, Train Acc: 8.06%, Test Loss: 3.5855, Test Acc: 15.20%, Time: 333.16s\n",
            "New best model saved with accuracy: 15.20%\n",
            "\n",
            "Epoch [3/100] - Learning Rate: 0.006000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 196/196 [05:34<00:00,  1.70s/it, loss=3.7, acc=12.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.7014, Train Acc: 12.34%, Test Loss: 3.4284, Test Acc: 18.24%, Time: 334.15s\n",
            "New best model saved with accuracy: 18.24%\n",
            "\n",
            "Epoch [4/100] - Learning Rate: 0.008000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 196/196 [05:26<00:00,  1.67s/it, loss=3.54, acc=15.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.5365, Train Acc: 15.18%, Test Loss: 3.1685, Test Acc: 22.52%, Time: 327.00s\n",
            "New best model saved with accuracy: 22.52%\n",
            "\n",
            "Epoch [5/100] - Learning Rate: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 196/196 [05:32<00:00,  1.70s/it, loss=3.43, acc=16.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.4302, Train Acc: 16.57%, Test Loss: 3.1089, Test Acc: 23.67%, Time: 332.47s\n",
            "New best model saved with accuracy: 23.67%\n",
            "\n",
            "Epoch [6/100] - Learning Rate: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     | 94/196 [02:39<02:53,  1.70s/it, loss=3.34, acc=18.4]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-737c3f2c6d5f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-737c3f2c6d5f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_centralized_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-737c3f2c6d5f>\u001b[0m in \u001b[0;36mrun_centralized_benchmark\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_centralized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-737c3f2c6d5f>\u001b[0m in \u001b[0;36mtrain_centralized_model\u001b[0;34m(model_name, dataset_name, batch_size, epochs, lr, momentum, weight_decay, lr_scheduler, save_path)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         train_loss, train_acc = train_epoch(\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-6-737c3f2c6d5f>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-737c3f2c6d5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# First conv block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Second conv block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}