{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdTC0ns/GWFrnl9o13gbBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zurehma/AML_Project/blob/zurehma/LAMB_1024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-optimizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv04IuqRpI3q",
        "outputId": "e833dd64-eea2-41e8-8257-f1184af3c780"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-optimizer in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (2.6.0+cu124)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch-optimizer) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x10SLwjFpA04",
        "outputId": "ef79a440-1c57-46e3-b0d3-1e0d050fa0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Using device: cpu\n",
            "Running LAMB benchmark with improved_cnn on cifar100...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 62.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 1,774,052\n",
            "\n",
            "Epoch [1/100] - Learning Rate: 0.004000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:34<00:00,  6.82s/it, loss=4.52, acc=2.47]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.5167, Train Acc: 2.47%, Test Loss: 4.2445, Test Acc: 5.83%, Time: 334.36s\n",
            "New best model saved with accuracy: 5.83%\n",
            "\n",
            "Epoch [2/100] - Learning Rate: 0.008000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:31<00:00,  6.76s/it, loss=4.14, acc=6.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.1388, Train Acc: 6.38%, Test Loss: 3.7249, Test Acc: 12.86%, Time: 331.11s\n",
            "New best model saved with accuracy: 12.86%\n",
            "\n",
            "Epoch [3/100] - Learning Rate: 0.012000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:21<00:00,  6.56s/it, loss=3.81, acc=10.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.8114, Train Acc: 10.38%, Test Loss: 3.4320, Test Acc: 18.03%, Time: 321.39s\n",
            "New best model saved with accuracy: 18.03%\n",
            "\n",
            "Epoch [4/100] - Learning Rate: 0.016000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:29<00:00,  6.73s/it, loss=3.62, acc=13.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6227, Train Acc: 13.66%, Test Loss: 3.3492, Test Acc: 19.02%, Time: 329.63s\n",
            "New best model saved with accuracy: 19.02%\n",
            "\n",
            "Epoch [5/100] - Learning Rate: 0.020000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:21<00:00,  6.56s/it, loss=3.53, acc=14.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.5290, Train Acc: 14.87%, Test Loss: 3.1213, Test Acc: 22.68%, Time: 321.62s\n",
            "New best model saved with accuracy: 22.68%\n",
            "\n",
            "Epoch [6/100] - Learning Rate: 0.024000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:33<00:00,  6.81s/it, loss=3.45, acc=16.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.4505, Train Acc: 16.18%, Test Loss: 3.3635, Test Acc: 19.31%, Time: 333.62s\n",
            "\n",
            "Epoch [7/100] - Learning Rate: 0.028000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 49/49 [05:20<00:00,  6.54s/it, loss=3.38, acc=17.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.3842, Train Acc: 17.66%, Test Loss: 3.0088, Test Acc: 24.61%, Time: 320.27s\n",
            "New best model saved with accuracy: 24.61%\n",
            "\n",
            "Epoch [8/100] - Learning Rate: 0.032000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  61%|██████    | 30/49 [03:35<02:16,  7.18s/it, loss=3.31, acc=18.3]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fc254c5d251c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-fc254c5d251c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_centralized_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-fc254c5d251c>\u001b[0m in \u001b[0;36mrun_centralized_benchmark\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_centralized_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-fc254c5d251c>\u001b[0m in \u001b[0;36mtrain_centralized_model\u001b[0;34m(model_name, dataset_name, batch_size, epochs, lr, momentum, weight_decay, lr_scheduler, save_path)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         train_loss, train_acc = train_epoch(\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-1-fc254c5d251c>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ─── Import LAMB from torch-optimizer ───────────────────────────────────────\n",
        "from torch_optimizer import Lamb\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "import math\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Seeding for reproducibility\n",
        "seed = 2025\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# transforms\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# load data\n",
        "def load_data(dataset_name=\"cifar100\", batch_size=128, num_workers=2):\n",
        "    \"\"\"Enhanced data loading with stronger augmentation for CIFAR-100\"\"\"\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "    test_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if dataset_name.lower() == \"cifar100\":\n",
        "        train_dataset = CIFAR100(\n",
        "            root=\"./data\", train=True, download=True, transform=train_transform\n",
        "        )\n",
        "        test_dataset = CIFAR100(\n",
        "            root=\"./data\", train=False, download=True, transform=test_transform\n",
        "        )\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset {dataset_name} not supported in enhanced version\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return train_loader, test_loader, num_classes\n",
        "\n",
        "\n",
        "# model - LeNet5 from the paper [11]\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Input: 3x32x32 (CIFAR has 3 color channels)\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)  # Output: 6x28x28\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # Output: 16x10x10\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 16x5x5\n",
        "\n",
        "        # fc layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # using BatchNorm for better training stability\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 384)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        # Second conv block\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        # Flatten and FC\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(model_name, num_classes=100):\n",
        "    if model_name.lower() == \"improved_cnn\":\n",
        "        return ImprovedCNN(num_classes=num_classes)\n",
        "    elif model_name.lower() == \"lenet5\":\n",
        "        return LeNet5(num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "\n",
        "# train\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        pbar.set_postfix({\"loss\": running_loss / total,\n",
        "                          \"acc\": 100.0 * correct / total})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100.0 * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# eval\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = 100.0 * correct / total\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "# train config\n",
        "def train_centralized_model(\n",
        "    model_name=\"improved_cnn\",\n",
        "    dataset_name=\"cifar100\",\n",
        "    batch_size=1024,\n",
        "    epochs=100,\n",
        "    lr=0.01,\n",
        "    momentum=0.9,\n",
        "    weight_decay=4e-4,\n",
        "    lr_scheduler=\"lamb\",\n",
        "    save_path=\"improved_cnn_model.pth\",\n",
        "):\n",
        "    # Load data\n",
        "    train_loader, test_loader, num_classes = load_data(\n",
        "        dataset_name=dataset_name, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(model_name, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Print total parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Linear‐scaling rule for LR\n",
        "    scaled_lr = lr * (batch_size / 256)\n",
        "\n",
        "    # Separate parameters into decay vs. no‐decay groups\n",
        "    decay_params = []\n",
        "    no_decay_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if param.ndim == 1:\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    # ─── Instantiate LAMB ─────────────────────────────────────────────────────────\n",
        "    #\n",
        "    # We set global weight_decay=0.0 because we're handling it per‐group.\n",
        "    #\n",
        "    optimizer = Lamb(\n",
        "        [\n",
        "            {\"params\": decay_params,    \"weight_decay\": weight_decay},\n",
        "            {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
        "        ],\n",
        "        lr=scaled_lr,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-6,\n",
        "        weight_decay=0.0  # global default; per-group overrides actual decay\n",
        "    )\n",
        "    # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # Learning‐rate scheduler: warmup + cosine, same as LARS code\n",
        "    if lr_scheduler == \"lamb\":\n",
        "        warmup_epochs = 10\n",
        "\n",
        "        def lr_lambda(epoch):\n",
        "            if epoch < warmup_epochs:\n",
        "                return float(epoch + 1) / float(warmup_epochs)\n",
        "            return 0.5 * (\n",
        "                1.0\n",
        "                + math.cos(\n",
        "                    math.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    elif lr_scheduler == \"step\":\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=[int(epochs * 0.5), int(epochs * 0.75)],\n",
        "            gamma=0.1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler: {lr_scheduler}\")\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{epochs}] - Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "            f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, \"\n",
        "            f\"Time: {train_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"test_acc\": test_acc,\n",
        "                },\n",
        "                save_path,\n",
        "            )\n",
        "            print(f\"New best model saved with accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"Best model loaded with accuracy: {checkpoint['test_acc']:.2f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# visualization of model performance\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    ax1.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    ax1.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Loss Curves\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    ax2.plot(history[\"test_acc\"], label=\"Test Accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy (%)\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Accuracy Curves\")\n",
        "    ax2.grid(True)\n",
        "\n",
        "    ax3.plot(history[\"lr\"])\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Learning Rate\")\n",
        "    ax3.set_title(\"Learning Rate Schedule\")\n",
        "    ax3.set_yscale(\"log\")\n",
        "    ax3.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# run benchmark\n",
        "def run_centralized_benchmark():\n",
        "    config = {\n",
        "        \"model_name\": \"improved_cnn\",\n",
        "        \"dataset_name\": \"cifar100\",\n",
        "        \"batch_size\": 1024,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"momentum\": 0.9,\n",
        "        \"weight_decay\": 4e-4,\n",
        "        \"lr_scheduler\": \"lamb\",\n",
        "        \"save_path\": \"improved_cnn_model_lamb.pth\",\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        f\"Running LAMB benchmark with {config['model_name']} on {config['dataset_name']}...\"\n",
        "    )\n",
        "\n",
        "    model, history = train_centralized_model(**config)\n",
        "    plot_training_history(history)\n",
        "\n",
        "    with open(\"history_lamb.pkl\", \"wb\") as f:\n",
        "        pickle.dump(history, f)\n",
        "\n",
        "    print(\"LAMB benchmark completed!\")\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def main():\n",
        "    model, history = run_centralized_benchmark()\n",
        "    return model, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}