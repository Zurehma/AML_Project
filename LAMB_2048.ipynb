{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4s7wtSihMpGbGPpGHWyKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zurehma/AML_Project/blob/zurehma/LAMB_2048.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-optimizer"
      ],
      "metadata": {
        "id": "NXyk6PmVw6X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqQ4KqXLwvW_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ─── Import LAMB from torch-optimizer ───────────────────────────────────────\n",
        "from torch_optimizer import Lamb\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "import math\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Seeding for reproducibility\n",
        "seed = 2025\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# transforms\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# load data\n",
        "def load_data(dataset_name=\"cifar100\", batch_size=128, num_workers=2):\n",
        "    \"\"\"Enhanced data loading with stronger augmentation for CIFAR-100\"\"\"\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "    test_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if dataset_name.lower() == \"cifar100\":\n",
        "        train_dataset = CIFAR100(\n",
        "            root=\"./data\", train=True, download=True, transform=train_transform\n",
        "        )\n",
        "        test_dataset = CIFAR100(\n",
        "            root=\"./data\", train=False, download=True, transform=test_transform\n",
        "        )\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset {dataset_name} not supported in enhanced version\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return train_loader, test_loader, num_classes\n",
        "\n",
        "\n",
        "# model - LeNet5 from the paper [11]\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Input: 3x32x32 (CIFAR has 3 color channels)\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)  # Output: 6x28x28\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # Output: 16x10x10\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 16x5x5\n",
        "\n",
        "        # fc layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # using BatchNorm for better training stability\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 384)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        # Second conv block\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        # Flatten and FC\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(model_name, num_classes=100):\n",
        "    if model_name.lower() == \"improved_cnn\":\n",
        "        return ImprovedCNN(num_classes=num_classes)\n",
        "    elif model_name.lower() == \"lenet5\":\n",
        "        return LeNet5(num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "\n",
        "# train\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        pbar.set_postfix({\"loss\": running_loss / total,\n",
        "                          \"acc\": 100.0 * correct / total})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100.0 * correct / total\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# eval\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = 100.0 * correct / total\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "# train config\n",
        "def train_centralized_model(\n",
        "    model_name=\"improved_cnn\",\n",
        "    dataset_name=\"cifar100\",\n",
        "    batch_size=2048,\n",
        "    epochs=100,\n",
        "    lr=0.01,\n",
        "    momentum=0.9,\n",
        "    weight_decay=4e-4,\n",
        "    lr_scheduler=\"lamb\",\n",
        "    save_path=\"improved_cnn_model.pth\",\n",
        "):\n",
        "    # Load data\n",
        "    train_loader, test_loader, num_classes = load_data(\n",
        "        dataset_name=dataset_name, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(model_name, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Print total parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Linear‐scaling rule for LR\n",
        "    scaled_lr = lr * (batch_size / 512)\n",
        "\n",
        "    # Separate parameters into decay vs. no‐decay groups\n",
        "    decay_params = []\n",
        "    no_decay_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if param.ndim == 1:\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    # ─── Instantiate LAMB ─────────────────────────────────────────────────────────\n",
        "    #\n",
        "    # We set global weight_decay=0.0 because we're handling it per‐group.\n",
        "    #\n",
        "    optimizer = Lamb(\n",
        "        [\n",
        "            {\"params\": decay_params,    \"weight_decay\": weight_decay},\n",
        "            {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
        "        ],\n",
        "        lr=scaled_lr,\n",
        "        betas=(0.9, 0.999),\n",
        "        eps=1e-6,\n",
        "        weight_decay=0.0  # global default; per-group overrides actual decay\n",
        "    )\n",
        "    # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "    # Learning‐rate scheduler: warmup + cosine, same as LARS code\n",
        "    if lr_scheduler == \"lamb\":\n",
        "        warmup_epochs = 10\n",
        "\n",
        "        def lr_lambda(epoch):\n",
        "            if epoch < warmup_epochs:\n",
        "                return float(epoch + 1) / float(warmup_epochs)\n",
        "            return 0.5 * (\n",
        "                1.0\n",
        "                + math.cos(\n",
        "                    math.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    elif lr_scheduler == \"step\":\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=[int(epochs * 0.5), int(epochs * 0.75)],\n",
        "            gamma=0.1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler: {lr_scheduler}\")\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{epochs}] - Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "            f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, \"\n",
        "            f\"Time: {train_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"test_acc\": test_acc,\n",
        "                },\n",
        "                save_path,\n",
        "            )\n",
        "            print(f\"New best model saved with accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"Best model loaded with accuracy: {checkpoint['test_acc']:.2f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# visualization of model performance\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    ax1.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    ax1.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Loss Curves\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    ax2.plot(history[\"test_acc\"], label=\"Test Accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy (%)\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Accuracy Curves\")\n",
        "    ax2.grid(True)\n",
        "\n",
        "    ax3.plot(history[\"lr\"])\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Learning Rate\")\n",
        "    ax3.set_title(\"Learning Rate Schedule\")\n",
        "    ax3.set_yscale(\"log\")\n",
        "    ax3.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# run benchmark\n",
        "def run_centralized_benchmark():\n",
        "    config = {\n",
        "        \"model_name\": \"improved_cnn\",\n",
        "        \"dataset_name\": \"cifar100\",\n",
        "        \"batch_size\": 2048,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"momentum\": 0.9,\n",
        "        \"weight_decay\": 4e-4,\n",
        "        \"lr_scheduler\": \"lamb\",\n",
        "        \"save_path\": \"improved_cnn_model_lamb.pth\",\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        f\"Running LAMB benchmark with {config['model_name']} on {config['dataset_name']}...\"\n",
        "    )\n",
        "\n",
        "    model, history = train_centralized_model(**config)\n",
        "    plot_training_history(history)\n",
        "\n",
        "    with open(\"history_lamb.pkl\", \"wb\") as f:\n",
        "        pickle.dump(history, f)\n",
        "\n",
        "    print(\"LAMB benchmark completed!\")\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def main():\n",
        "    model, history = run_centralized_benchmark()\n",
        "    return model, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}