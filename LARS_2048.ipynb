{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKwAcfXfzC3oTN6nxLcqOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zurehma/AML_Project/blob/zurehma/LARS_2048.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PTNLTKDyXbgw",
        "outputId": "05cee1ab-6c29-40e2-f53f-da1ef42b2ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-optimizer\n",
            "  Using cached torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (2.5.1+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch-optimizer)\n",
            "  Using cached pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch-optimizer) (3.0.2)\n",
            "Using cached torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "Using cached pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch-optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10, CIFAR100\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch_optimizer import LARS\n",
        "\n",
        "import math\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Seeding for reproducibility\n",
        "seed = 2025\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# transforms\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# load data\n",
        "\n",
        "\n",
        "def load_data(dataset_name=\"cifar100\", batch_size=128, num_workers=2):\n",
        "    \"\"\"Enhanced data loading with stronger augmentation for CIFAR-100\"\"\"\n",
        "\n",
        "    # Better augmentation for CIFAR-100\n",
        "    train_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    test_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if dataset_name.lower() == \"cifar100\":\n",
        "        train_dataset = CIFAR100(\n",
        "            root=\"./data\", train=True, download=True, transform=train_transform\n",
        "        )\n",
        "        test_dataset = CIFAR100(\n",
        "            root=\"./data\", train=False, download=True, transform=test_transform\n",
        "        )\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset {dataset_name} not supported in enhanced version\")\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, num_classes\n",
        "\n",
        "\n",
        "# model - LeNet5 from the paper [11]\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Input: 3x32x32 (CIFAR has 3 color channels)\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)  # Output: 6x28x28\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # Output: 16x10x10\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 16x5x5\n",
        "\n",
        "        # fc layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # using BatchNorm for better training stability\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 384)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "\n",
        "        # Second conv block\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        # Flatten and fully connected layers\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(model_name, num_classes=100):\n",
        "    if model_name.lower() == \"improved_cnn\":\n",
        "        return ImprovedCNN(num_classes=num_classes)\n",
        "    elif model_name.lower() == \"lenet5\":\n",
        "        return LeNet5(num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "\n",
        "# train\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # just a progress bar\n",
        "        pbar.set_postfix({\"loss\": running_loss / total, \"acc\": 100.0 * correct / total})\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100.0 * correct / total\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# eval\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = 100.0 * correct / total\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "# train config\n",
        "def train_centralized_model(\n",
        "    model_name=\"improved_cnn\",\n",
        "    dataset_name=\"cifar100\",\n",
        "    batch_size=2048,\n",
        "    epochs=100,\n",
        "    lr=0.01,\n",
        "    momentum=0.9,\n",
        "    weight_decay=4e-4,\n",
        "    lr_scheduler=\"lars\",\n",
        "    save_path=\"improved_cnn_model.pth\",\n",
        "):\n",
        "    # Load data\n",
        "    train_loader, test_loader, num_classes = load_data(\n",
        "        dataset_name=dataset_name, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(model_name, num_classes=num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    scaled_lr = lr * (batch_size / 128)    # linear‐scaling rule\n",
        "\n",
        "    decay_params   = []\n",
        "    no_decay_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        # any 1-D param (bias, BatchNorm weight, etc.) => no decay\n",
        "        if param.ndim == 1:\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    optimizer = LARS(\n",
        "        [\n",
        "            {'params': decay_params,    'weight_decay': weight_decay},\n",
        "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
        "        ],\n",
        "        lr=scaled_lr,\n",
        "        momentum=momentum,\n",
        "        # set the *global* default decay to 0, since we've handled it per-group:\n",
        "        weight_decay=0.0,\n",
        "        trust_coefficient=0.05\n",
        "    )\n",
        "\n",
        "    # lr\n",
        "    if lr_scheduler == \"lars\":\n",
        "        warmup_epochs = 5\n",
        "        def lr_lambda(epoch):\n",
        "            # linear warmup\n",
        "            if epoch < warmup_epochs:\n",
        "                return float(epoch + 1) / float(warmup_epochs)\n",
        "            # then cosine decay\n",
        "            return 0.5 * (\n",
        "                1.0\n",
        "                + math.cos(\n",
        "                    math.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    elif lr_scheduler == \"step\":\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer,\n",
        "            milestones=[int(epochs * 0.5), int(epochs * 0.75)],\n",
        "            gamma=0.1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler: {lr_scheduler}\")\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    # Track best accuracy\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{epochs}] - Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Training for one epoch\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "        # Step learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print statistics\n",
        "        print(\n",
        "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "            f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, \"\n",
        "            f\"Time: {train_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"test_acc\": test_acc,\n",
        "                },\n",
        "                save_path,\n",
        "            )\n",
        "            print(f\"New best model saved with accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "        # Update history\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"Best model loaded with accuracy: {checkpoint['test_acc']:.2f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# visualization of model performance\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # plot the loss\n",
        "    ax1.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "    ax1.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Loss Curves\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # plot accuracy\n",
        "    ax2.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    ax2.plot(history[\"test_acc\"], label=\"Test Accuracy\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Accuracy (%)\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Accuracy Curves\")\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # plot learning rate\n",
        "    ax3.plot(history[\"lr\"])\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Learning Rate\")\n",
        "    ax3.set_title(\"Learning Rate Schedule\")\n",
        "    ax3.set_yscale(\"log\")\n",
        "    ax3.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# run benchmark\n",
        "def run_centralized_benchmark():\n",
        "    # LeNet5 config\n",
        "    config = {\n",
        "        \"model_name\": \"improved_cnn\",\n",
        "        \"dataset_name\": \"cifar100\",\n",
        "        \"batch_size\": 2048,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"momentum\": 0.9,\n",
        "        \"weight_decay\": 4e-4,\n",
        "        \"lr_scheduler\": \"lars\",\n",
        "        \"save_path\": \"improved_cnn_model.pth\",\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        f\"Running LARS benchmark with {config['model_name']} on {config['dataset_name']}...\"\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    model, history = train_centralized_model(**config)\n",
        "\n",
        "    # Plot results\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Save history\n",
        "    with open(\"lenet5_history.pkl\", \"wb\") as f:\n",
        "        pickle.dump(history, f)\n",
        "\n",
        "    print(\"LARS benchmark completed!\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def main():\n",
        "    model, history = run_centralized_benchmark()\n",
        "    return model, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "lRcSOwI8K3ei",
        "outputId": "e1ea90c6-65a3-4478-9f5c-f24e277fafa3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_optimizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d9aaf74f4b9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_optimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLARS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_optimizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}